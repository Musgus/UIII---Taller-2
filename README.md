# üåç Traducci√≥n Autom√°tica Neuronal (NMT) - Espa√±ol ‚Üí Ingl√©s

Proyecto completo de **Neural Machine Translation (NMT)** que implementa, entrena y compara **4 arquitecturas diferentes** de modelos encoder-decoder para traducci√≥n autom√°tica de espa√±ol a ingl√©s.

## üìã Contenido

- [Caracter√≠sticas](#-caracter√≠sticas)
- [Arquitecturas Implementadas](#-arquitecturas-implementadas)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Instalaci√≥n](#-instalaci√≥n)
- [Uso R√°pido](#-uso-r√°pido)
- [Pipeline Completo](#-pipeline-completo)
- [Resultados](#-resultados)
- [Documentaci√≥n T√©cnica](#-documentaci√≥n-t√©cnica)
- [Extensiones Futuras](#-extensiones-futuras)

## ‚ú® Caracter√≠sticas

### üéØ Modelos Implementados
- ‚úÖ **RNN Simple** (sin atenci√≥n) - Baseline
- ‚úÖ **LSTM Bidireccional** con Atenci√≥n Bahdanau
- ‚úÖ **GRU Bidireccional** con Atenci√≥n Bahdanau
- ‚úÖ **Transformer** (simplificado, 2 capas)

### üîß Funcionalidades
- üì¶ Descarga autom√°tica del dataset OPUS Tatoeba
- üßπ Preprocesamiento completo (limpieza, normalizaci√≥n, filtrado)
- üî§ Tokenizaci√≥n subword con **SentencePiece** (BPE)
- üíæ **Checkpointing autom√°tico** durante entrenamiento
- üìä **M√©tricas persistidas** (BLEU, loss, tiempos, par√°metros)
- üìà **Visualizaciones autom√°ticas** (curvas, comparaciones, tablas)
- üîÑ Early stopping y learning rate scheduling
- üé® An√°lisis detallado por longitud de oraci√≥n

## üèóÔ∏è Arquitecturas Implementadas

### 1Ô∏è‚É£ RNN Simple (Sin Atenci√≥n)
**Baseline** - Arquitectura encoder-decoder b√°sica
```
Encoder: Embedding ‚Üí RNN (2 capas) ‚Üí Vector contexto
Decoder: Vector contexto ‚Üí RNN (2 capas) ‚Üí Linear ‚Üí Softmax
```
- ‚öôÔ∏è Par√°metros: ~15M
- üì¶ Hidden dim: 512
- üéØ Uso: Comparaci√≥n baseline

### 2Ô∏è‚É£ LSTM con Atenci√≥n Bahdanau
**Arquitectura con memoria a largo plazo**
```
Encoder: Embedding ‚Üí BiLSTM (2 capas) ‚Üí Outputs
Decoder: Atenci√≥n(Outputs) + LSTM ‚Üí Linear ‚Üí Softmax
```
- ‚öôÔ∏è Par√°metros: ~20M
- üì¶ Hidden dim: 512
- üéØ Atenci√≥n: Bahdanau (additive)
- üí° Ventaja: Captura dependencias largas mejor que RNN

### 3Ô∏è‚É£ GRU con Atenci√≥n Bahdanau
**Similar a LSTM pero m√°s eficiente**
```
Encoder: Embedding ‚Üí BiGRU (2 capas) ‚Üí Outputs
Decoder: Atenci√≥n(Outputs) + GRU ‚Üí Linear ‚Üí Softmax
```
- ‚öôÔ∏è Par√°metros: ~18M (25% menos que LSTM)
- üì¶ Hidden dim: 512
- üéØ Atenci√≥n: Bahdanau (additive)
- üí° Ventaja: M√°s r√°pido que LSTM, similar rendimiento

### 4Ô∏è‚É£ Transformer
**Estado del arte en NMT**
```
Encoder: Embedding + Positional ‚Üí Multi-Head Self-Attention (2 capas)
Decoder: Embedding + Positional ‚Üí Masked Self-Attention + Cross-Attention (2 capas)
```
- ‚öôÔ∏è Par√°metros: ~25M
- üì¶ d_model: 256, heads: 8
- üéØ Capas: 2 encoder + 2 decoder
- üí° Ventaja: Paralelizable, mejor para dependencias largas

## üìÅ Estructura del Proyecto

```
Taller 2/
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias
‚îú‚îÄ‚îÄ main.py                        # Script principal (pipeline completo)
‚îÇ
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py          # Descarga dataset OPUS Tatoeba
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py             # Preprocesamiento y tokenizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                # PyTorch Dataset y DataLoader
‚îÇ   ‚îú‚îÄ‚îÄ model_rnn.py              # Modelo RNN simple
‚îÇ   ‚îú‚îÄ‚îÄ model_lstm_attention.py   # Modelo LSTM con atenci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ model_gru_attention.py    # Modelo GRU con atenci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ model_transformer.py      # Modelo Transformer
‚îÇ   ‚îú‚îÄ‚îÄ train.py                  # Sistema de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py               # Evaluaci√≥n con BLEU
‚îÇ   ‚îú‚îÄ‚îÄ visualize.py              # Visualizaci√≥n de resultados
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Utilidades generales
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Datos
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Corpus crudo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parallel_corpus.tsv
‚îÇ   ‚îî‚îÄ‚îÄ processed/                # Datos procesados
‚îÇ       ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ       ‚îú‚îÄ‚îÄ valid.jsonl
‚îÇ       ‚îú‚îÄ‚îÄ test.jsonl
‚îÇ       ‚îî‚îÄ‚îÄ metadata.json
‚îÇ
‚îú‚îÄ‚îÄ artifacts/                     # Artefactos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer/                # Modelos SentencePiece
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spm_es.model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spm_en.model
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Checkpoints de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RNN_Simple/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ best_model.pt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ last_checkpoint.pt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ architecture.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LSTM_Attention/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GRU_Attention/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Transformer/
‚îÇ   ‚îî‚îÄ‚îÄ logs/                     # Logs de entrenamiento
‚îÇ
‚îî‚îÄ‚îÄ resultados/                    # Resultados finales
    ‚îú‚îÄ‚îÄ metrics/                   # M√©tricas por modelo
    ‚îÇ   ‚îú‚îÄ‚îÄ RNN_Simple/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_metrics.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ all_translations.jsonl
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ translation_examples.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ LSTM_Attention/
    ‚îÇ   ‚îú‚îÄ‚îÄ GRU_Attention/
    ‚îÇ   ‚îî‚îÄ‚îÄ Transformer/
    ‚îÇ
    ‚îî‚îÄ‚îÄ plots/                     # Visualizaciones
        ‚îú‚îÄ‚îÄ RNN_Simple_training_curves.png
        ‚îú‚îÄ‚îÄ LSTM_Attention_training_curves.png
        ‚îú‚îÄ‚îÄ GRU_Attention_training_curves.png
        ‚îú‚îÄ‚îÄ Transformer_training_curves.png
        ‚îú‚îÄ‚îÄ all_models_training_comparison.png
        ‚îú‚îÄ‚îÄ bleu_comparison.png
        ‚îú‚îÄ‚îÄ bleu_by_length.png
        ‚îú‚îÄ‚îÄ models_comparison_table.png
        ‚îî‚îÄ‚îÄ training_time_comparison.png
```

## üöÄ Instalaci√≥n

### Requisitos
- Python 3.8+
- PyTorch 2.0+
- 8GB+ RAM (16GB+ recomendado)
- GPU con 6GB+ VRAM (opcional pero recomendado)

### Paso 1: Clonar el repositorio
```bash
cd "Taller 2"
```

### Paso 2: Instalar dependencias
```bash
pip install -r requirements.txt
```

### Dependencias principales:
- `torch` - Framework de deep learning
- `sentencepiece` - Tokenizaci√≥n subword
- `sacrebleu` - C√°lculo de BLEU
- `matplotlib`, `seaborn` - Visualizaci√≥n
- `tqdm` - Barras de progreso

## üéØ Uso R√°pido

### Opci√≥n 1: Pipeline Completo Autom√°tico
```bash
# Ejecuta TODO: descarga, preprocesamiento, entrenamiento, evaluaci√≥n y visualizaci√≥n
python main.py
```

### Opci√≥n 2: Paso a Paso

#### 1. Descargar y preparar datos
```bash
python src/download_data.py
```
**Nota**: Si la descarga autom√°tica falla, sigue las instrucciones para descarga manual.

#### 2. Preprocesar datos
```bash
python src/preprocess.py
```
Genera:
- Corpus limpio y filtrado
- Tokenizers SentencePiece entrenados
- Splits train/valid/test (80/10/10)

#### 3. Entrenar modelos
```bash
python main.py
```

#### 4. Solo evaluar (si ya entrenaste)
```bash
python main.py --skip-training
```

#### 5. Solo visualizar resultados
```bash
python main.py --skip-training --skip-evaluation
```

## üìä Pipeline Completo

### Fase 1: Preparaci√≥n de Datos
```python
python src/download_data.py
python src/preprocess.py
```

**Transformaciones aplicadas:**
1. ‚úÖ Limpieza: min√∫sculas, normalizaci√≥n de espacios
2. ‚úÖ Filtrado: longitud 3-50 palabras, ratio m√°ximo 2.5:1
3. ‚úÖ Tokenizaci√≥n: SentencePiece BPE (vocab_size=16000)
4. ‚úÖ Split: 80% train, 10% valid, 10% test

### Fase 2: Entrenamiento
```python
python main.py
```

**Configuraci√≥n de entrenamiento:**
- Batch size: 64
- Learning rate: 0.001 (con ReduceLROnPlateau)
- √âpocas: 20 (con early stopping patience=5)
- Optimizador: Adam
- Loss: CrossEntropyLoss (ignora padding)
- Gradient clipping: 1.0
- Teacher forcing: 0.5 (con decaimiento exponencial)

**Guardado autom√°tico:**
- ‚úÖ Checkpoint cada √©poca
- ‚úÖ Mejor modelo (best_model.pt)
- ‚úÖ √öltimo checkpoint (last_checkpoint.pt)
- ‚úÖ M√©tricas en JSON

### Fase 3: Evaluaci√≥n
```python
# Autom√°tico en main.py o manual:
python -c "from evaluate import *; # c√≥digo evaluaci√≥n"
```

**M√©tricas calculadas:**
- üìä BLEU Score global
- üìä BLEU por longitud (corta/media/larga)
- üìä An√°lisis de errores de longitud
- üìù 50 ejemplos de traducci√≥n guardados
- üíæ Todas las traducciones en JSONL

### Fase 4: Visualizaci√≥n
```python
# Autom√°tico en main.py o manual con visualize.py
```

**Gr√°ficos generados:**
1. Curvas de loss individuales (train/valid)
2. Comparaci√≥n de loss entre modelos
3. Comparaci√≥n de BLEU (gr√°fico de barras)
4. BLEU por longitud de oraci√≥n
5. Tabla comparativa completa
6. Comparaci√≥n de tiempos de entrenamiento

## üìà Resultados Esperados

### Tabla Comparativa (Ejemplo con Tatoeba es-en)

| Modelo | Par√°metros | BLEU | Valid Loss | Tiempo | Observaciones |
|--------|-----------|------|------------|--------|---------------|
| **Transformer** | ~25M | **35-45** | 2.5 | 45 min | ü•á Mejor BLEU |
| **LSTM Attention** | ~20M | 32-42 | 2.8 | 60 min | ü•à Buen balance |
| **GRU Attention** | ~18M | 31-41 | 2.9 | 50 min | M√°s r√°pido que LSTM |
| **RNN Simple** | ~15M | 25-35 | 3.5 | 40 min | Baseline |

**Nota**: Resultados dependen del tama√±o del corpus y recursos computacionales.

### BLEU por Longitud de Oraci√≥n

```
Oraciones Cortas (‚â§10 palabras):  BLEU ~40-50
Oraciones Medias (11-20 palabras): BLEU ~30-40
Oraciones Largas (>20 palabras):  BLEU ~20-30
```

## üìö Documentaci√≥n T√©cnica

### Dataset: OPUS Tatoeba
- **Fuente**: https://opus.nlpl.eu/Tatoeba.php
- **Par de idiomas**: Espa√±ol (es) ‚Üí Ingl√©s (en)
- **Tama√±o**: ~100,000 pares de oraciones
- **Caracter√≠sticas**:
  - Oraciones cortas y medianas
  - Alta calidad
  - Dominio general

### Tokenizaci√≥n: SentencePiece
- **Algoritmo**: Byte Pair Encoding (BPE)
- **Vocab size**: 16,000 tokens
- **Character coverage**: 99.95%
- **Tokens especiales**:
  - `<pad>` (ID: 0)
  - `<bos>` (ID: 1) - Inicio de secuencia
  - `<eos>` (ID: 2) - Fin de secuencia
  - `<unk>` (ID: 3) - Token desconocido

### M√©tricas de Evaluaci√≥n

#### BLEU Score (sacreBLEU)
```python
from sacrebleu.metrics import BLEU
bleu = BLEU()
score = bleu.corpus_score(hypotheses, [references])
```
- **Rango**: 0-100
- **Interpretaci√≥n**:
  - BLEU < 20: Malo
  - BLEU 20-30: Aceptable
  - BLEU 30-40: Bueno
  - BLEU > 40: Muy bueno

### Comparaci√≥n de Arquitecturas

#### RNN vs LSTM vs GRU
| Caracter√≠stica | RNN | LSTM | GRU |
|---------------|-----|------|-----|
| **Cell state** | No | S√≠ | No |
| **Puertas** | 0 | 3 (input, forget, output) | 2 (update, reset) |
| **Par√°metros** | Menos | M√°s | Medio |
| **Entrenamiento** | R√°pido | Lento | Medio |
| **Vanishing gradient** | S√≠ | No | No |

#### RNN/LSTM/GRU vs Transformer
| Caracter√≠stica | RNN/LSTM/GRU | Transformer |
|---------------|--------------|-------------|
| **Procesamiento** | Secuencial | Paralelo |
| **Dependencias largas** | Limitado | Excelente |
| **Velocidad entrenamiento** | Lenta | R√°pida (con GPU) |
| **Memoria** | Menos | M√°s |
| **Estado del arte** | No | S√≠ |

## üéì Conceptos Clave

### Atenci√≥n Bahdanau
Permite al decoder "mirar" diferentes partes del input en cada paso:

```python
score(h_t, h_s) = v^T * tanh(W_1*h_t + W_2*h_s)
attention_weights = softmax(scores)
context = Œ£(attention_weights * encoder_outputs)
```

**Ventajas:**
- ‚úÖ Alinea autom√°ticamente source y target
- ‚úÖ Resuelve bottleneck del vector de contexto fijo
- ‚úÖ Visualizable (mapas de atenci√≥n)

### Teacher Forcing
Durante entrenamiento, usa el ground truth como input del decoder:

```python
if random() < teacher_forcing_ratio:
    decoder_input = target_token  # Ground truth
else:
    decoder_input = predicted_token  # Predicci√≥n
```

**Decaimiento exponencial:**
```python
ratio_epoch_t = ratio_inicial * (0.95 ** (epoch - 1))
```

### Positional Encoding (Transformer)
Inyecta informaci√≥n de posici√≥n:

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

## üîß Configuraci√≥n Avanzada

### Modificar Hiperpar√°metros
Edita `config.py`:

```python
# Cambiar tama√±o de batch
BATCH_SIZE = 128  # Default: 64

# Cambiar learning rate
LEARNING_RATE = 0.0005  # Default: 0.001

# Cambiar √©pocas
NUM_EPOCHS = 30  # Default: 20

# Cambiar vocabulario
VOCAB_SIZE = 32000  # Default: 16000
```

### Cambiar Arquitectura
Para modificar una arquitectura, edita el archivo correspondiente:
- `src/model_rnn.py`
- `src/model_lstm_attention.py`
- `src/model_gru_attention.py`
- `src/model_transformer.py`

Ejemplo (aumentar capas de Transformer):
```python
TRANSFORMER_CONFIG = {
    "d_model": 512,           # Default: 256
    "num_encoder_layers": 4,  # Default: 2
    "num_decoder_layers": 4,  # Default: 2
}
```

## üêõ Troubleshooting

### Error: Out of Memory (GPU)
```bash
# Reducir batch size en config.py
BATCH_SIZE = 32  # o menos

# O usar acumulaci√≥n de gradientes
```

### Error: Dataset no encontrado
```bash
# Descargar manualmente desde:
# https://opus.nlpl.eu/Tatoeba.php
# Colocar en: data/raw/
```

### Error: CUDA not available
```bash
# Verificar instalaci√≥n de PyTorch con CUDA:
python -c "import torch; print(torch.cuda.is_available())"

# Instalar PyTorch con CUDA:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Entrenamiento muy lento
```bash
# 1. Verificar que est√© usando GPU
python -c "import config; print(config.DEVICE)"

# 2. Reducir complejidad del modelo
# Editar config.py: reducir hidden_dim, num_layers

# 3. Entrenar con menos datos
# Editar preprocess.py: tomar subset del corpus
```

## üöÄ Extensiones Futuras

### 1. Mejoras de Modelo
- [ ] Beam search (actualmente solo greedy)
- [ ] Label smoothing
- [ ] Byte-level BPE
- [ ] Modelos pre-entrenados (mBART, mT5)

### 2. M√°s Pares de Idiomas
```python
# En config.py
SOURCE_LANG = "de"  # Alem√°n
TARGET_LANG = "en"  # Ingl√©s
```

### 3. Dataset M√°s Grande
- [ ] OPUS-100 (~55M pares)
- [ ] WMT datasets
- [ ] ParaCrawl

### 4. T√©cnicas Avanzadas
- [ ] Back-translation
- [ ] Multi-task learning
- [ ] Domain adaptation
- [ ] Low-resource NMT

### 5. Deployment
- [ ] API REST con FastAPI
- [ ] Modelo ONNX para inferencia
- [ ] Cuantizaci√≥n para mobile
- [ ] Docker container

## üìñ Referencias

### Papers Fundamentales
1. **Sequence to Sequence Learning** (Sutskever et al., 2014)
2. **Neural Machine Translation by Jointly Learning to Align and Translate** (Bahdanau et al., 2014)
3. **Attention is All You Need** (Vaswani et al., 2017)

### Recursos √ötiles
- [OPUS Corpus](https://opus.nlpl.eu/)
- [SentencePiece](https://github.com/google/sentencepiece)
- [sacreBLEU](https://github.com/mjpost/sacrebleu)
- [PyTorch Seq2Seq Tutorials](https://github.com/bentrevett/pytorch-seq2seq)

## üìù Licencia

Este proyecto es c√≥digo educativo de c√≥digo abierto.

## üë§ Autor

Proyecto desarrollado para el curso de IA III - Traducci√≥n Autom√°tica Neuronal.

---

## üéØ Comandos R√°pidos

```bash
# Pipeline completo (recomendado)
python main.py

# Solo descarga de datos
python src/download_data.py

# Solo preprocesamiento
python src/preprocess.py

# Entrenar solo un modelo espec√≠fico
# (editar main.py para comentar otros modelos)

# Evaluar modelos existentes
python main.py --skip-training

# Solo visualizaciones
python main.py --skip-training --skip-evaluation

# Ver informaci√≥n del dispositivo
python -c "from src.utils import print_device_info; print_device_info()"

# Verificar datos preparados
python -c "from main import check_data_ready; check_data_ready()"
```

---

**¬°Buena suerte con tu proyecto de traducci√≥n autom√°tica neuronal! üöÄüåç**
